{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_1_prueba.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfJnVw/0PfyPwVSQsRgIvP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoncaloCastroCarneirinho/Keras/blob/main/keras_1_prueba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhfwUKKvdTA_"
      },
      "source": [
        "Just one hidden layer in Deep Learning models, means that the relationship between inputs and outputs is linear. One hidden layer means we have a just neural network; having two ore more hidden layers means we have a deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUXcH7btfkCm"
      },
      "source": [
        "Every layer nodes are connected to the followers through its one and exclusive weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TCZSBuLdTVl"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Zo5t4bieN7"
      },
      "source": [
        "importing dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v2gKFprfdTdL",
        "outputId": "1b33d009-ac26-4c86-f65b-66f7566f758a"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist #28x28 images of hand-written digits 0-9\n",
        "                                #MNIST is a Keras dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() #unpacking dataset to training and testing variables\n",
        "\n",
        "#We can scale or normalize the image data colors\n",
        "#This makes easy for network to learn\n",
        "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
        "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
        "\n",
        "#We can build our model\n",
        "model = tf.keras.models.Sequential() #######SEQUENTIAL, WHY?????\n",
        "model.add(tf.keras.layers.Flatten()) #Adding the first layer (input) which is 28x28\n",
        "#When we flat an image dimensions, is like we are building a vector with (in this case) 28x28 = 784 dimension\n",
        "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) #128 as the first element of the 'activation' is the number of neurons on the layer\n",
        "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) #128 as the first element of the 'activation' is the number of neurons on the layer\n",
        "model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax)) #10 as the first element of the 'activation' is the number of classifications or outputs\n",
        "                                                                #softmax as probability distributions activation function\n",
        "\n",
        "#Now, we define the parameters of the model\n",
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy']) #loss is the degree of error\n",
        "#A neural network does not try to maximize accuracy, they try to minimize the error\n",
        "#'adam' is the default optimizer\n",
        "\n",
        "#Now, we train the model\n",
        "model.fit(x_train, y_train, epochs = 3)\n",
        "\n",
        "plt.imshow(x_train[34000], cmap = plt.cm.binary) #cmap and cm is color map. Binary means the image will be black & white\n",
        "plt.show()\n",
        "\n",
        "print(x_train[0])\n",
        "\n",
        "#Testing the model and evaluating the loss and the accuracy\n",
        "val_loss, val_acc = model.evaluate(x_test, y_test) #as in the training model, 'x' is the input (image) and 'y' is the output (corresponding value) \n",
        "print(val_loss, val_acc)\n",
        "\n",
        "#It is fundamental the test model to be less accurate and to have more error than the training model. \n",
        "#The testing and training accuracy and error values shall not vary to much.\n",
        "\n",
        "#Now, we are going to save a model...\n",
        "model.save('epic_num_reader.model')\n",
        "\n",
        "#We can reload the model into a new variable...\n",
        "new_model = tf.keras.models.load_model('epic_num_reader.model')\n",
        "\n",
        "#If we want to make a prediction of a testing model...\n",
        "predictions = new_model.predict([x_test])\n",
        "\n",
        "print(predictions) #I DO NOT UNDERSTAND WHY THIS IS A PROBABILITY TENSOR!!!!!\n",
        "\n",
        "#Now we can calculate the image number prediction\n",
        "import numpy as np\n",
        "\n",
        "print(np.argmax(predictions[9000]))\n",
        "\n",
        "#Now, we confirm the prediction\n",
        "plt.imshow(x_test[9000])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4664 - accuracy: 0.8674\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1097 - accuracy: 0.9655\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0702 - accuracy: 0.9780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.00393124 0.02332955 0.02620568 0.02625207 0.17420356 0.17566281\n",
            "  0.28629534 0.05664824 0.51877786 0.71632322 0.77892406 0.89301644\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.05780486 0.06524513 0.16128198 0.22713296\n",
            "  0.22277047 0.32790981 0.36833534 0.3689874  0.34978968 0.32678448\n",
            "  0.368094   0.3747499  0.79066747 0.67980478 0.61494005 0.45002403\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.12250613 0.45858525 0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.32790981 0.36833534 0.3689874  0.34978968 0.32420121\n",
            "  0.15214552 0.17865984 0.25626376 0.1573102  0.12298801 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.04500225 0.4219755  0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.32790981 0.28826244 0.26543758 0.34149427 0.31128482\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.1541463  0.28272888 0.18358693 0.37314701\n",
            "  0.33153488 0.26569767 0.01601458 0.         0.05945042 0.19891229\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.0253731  0.00171577 0.22713296\n",
            "  0.33153488 0.11664776 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.20500962\n",
            "  0.33153488 0.24625638 0.00291174 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.01622378\n",
            "  0.24897876 0.32790981 0.10191096 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.04586451 0.31235677 0.32757096 0.23335172 0.14931733 0.00129164\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.10498298 0.34940902 0.3689874  0.34978968 0.15370495\n",
            "  0.04089933 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.06551419 0.27127137 0.34978968 0.32678448\n",
            "  0.245396   0.05882702 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.02333517 0.12857881 0.32549285\n",
            "  0.41390126 0.40743158 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32161793\n",
            "  0.41390126 0.54251585 0.20001074 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.06697006 0.18959827 0.25300993 0.32678448\n",
            "  0.41390126 0.45100715 0.00625034 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.05110617 0.19182076 0.33339444 0.3689874  0.34978968 0.32678448\n",
            "  0.40899334 0.39653769 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.04117838 0.16813739\n",
            "  0.28960162 0.32790981 0.36833534 0.3689874  0.34978968 0.25961929\n",
            "  0.12760592 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.04431706 0.11961607 0.36545809 0.37314701\n",
            "  0.33153488 0.32790981 0.36833534 0.28877275 0.111988   0.00258328\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.05298497 0.42752138 0.4219755  0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.25273681 0.11646967 0.01312603 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.37491383 0.56222061\n",
            "  0.66525569 0.63253163 0.48748768 0.45852825 0.43408872 0.359873\n",
            "  0.17428513 0.01425695 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.92705966 0.82698729\n",
            "  0.74473314 0.63253163 0.4084877  0.24466922 0.22648107 0.02359823\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n",
            "313/313 [==============================] - 1s 1ms/step - loss: 0.0888 - accuracy: 0.9724\n",
            "0.08883512765169144 0.9724000096321106\n",
            "INFO:tensorflow:Assets written to: epic_num_reader.model/assets\n",
            "[[4.29432578e-09 1.75747339e-09 2.81586472e-06 ... 9.99848247e-01\n",
            "  2.40991991e-08 1.17889627e-07]\n",
            " [2.43724703e-08 1.15245120e-05 9.99952555e-01 ... 1.87641236e-09\n",
            "  1.76357469e-06 2.46283757e-11]\n",
            " [1.05482712e-07 9.98884261e-01 2.28167974e-05 ... 8.43276503e-04\n",
            "  1.13948176e-04 2.58459545e-06]\n",
            " ...\n",
            " [2.87850854e-09 7.08072207e-07 2.66996558e-09 ... 2.49161326e-06\n",
            "  4.73061027e-06 7.65616714e-05]\n",
            " [1.08689865e-05 7.68710748e-08 2.23130911e-07 ... 2.30796741e-06\n",
            "  1.30213521e-04 9.92628770e-08]\n",
            " [2.75313408e-08 1.83586568e-08 8.59333156e-08 ... 7.29760176e-12\n",
            "  1.94404990e-07 1.75564688e-10]]\n",
            "7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}